Project
========
Once the files are in your directory we read them
```{r,cache=TRUE}
training <- read.csv("pml-training.csv",header=TRUE)
testing<- read.csv("pml-testing.csv",header=TRUE)
```

Now we are going to see the data, so first i am gonna see the number of variables
```{r}
dim(training)
```
So we see that there are 160 variables
now i would like to look the class of the outcome variable  and the first rows and the last
```{r}
class(training$classe)
head(training$classe)
tail(training$classe)
```
So I see that there are at least two levels of thh data and i guess that are the letters through the A to E so i am going to transform the outcome as a factor, adn see the class the levels
```{r}
training$classe <- as.factor(training$classe)
class(training$classe)
str(training$classe)
```
Now let's look more information about our data
```{r,eval=FALSE}
head(training)
str(training)
```
now looking the first rows, i can see that are a lot of columns that have Nas values and dont have values, and the columns that dont have values are character, so if i transform it to a numeric type, those columns with null values are going to fill with Nas;
Now lets see how many Nas are in the columns and see if we can accept it
```{r,cache=TRUE,echo=FALSE}
training$kurtosis_roll_belt <- as.numeric(training$kurtosis_roll_belt)
training$kurtosis_picth_belt <- as.numeric(training$kurtosis_picth_belt)
training$kurtosis_yaw_belt  <- as.numeric(training$kurtosis_yaw_belt )
training$skewness_roll_belt <- as.numeric(training$skewness_roll_belt)
training$skewness_roll_belt.1 <- as.numeric(training$skewness_roll_belt.1)
training$skewness_yaw_belt  <- as.numeric(training$skewness_yaw_belt )
training$max_yaw_belt <- as.numeric(training$max_yaw_belt)
training$min_yaw_belt <- as.numeric(training$min_yaw_belt)
training$amplitude_yaw_belt <- as.numeric(training$amplitude_yaw_belt)
training$kurtosis_roll_arm <- as.numeric(training$kurtosis_roll_arm)
training$kurtosis_picth_arm<- as.numeric(training$kurtosis_picth_arm)
training$kurtosis_yaw_arm <- as.numeric(training$kurtosis_yaw_arm)
training$skewness_roll_arm <- as.numeric(training$skewness_roll_arm)
training$skewness_pitch_arm <- as.numeric(training$skewness_pitch_arm)
training$skewness_yaw_arm  <- as.numeric(training$skewness_yaw_arm )
training$kurtosis_roll_dumbbell  <- as.numeric(training$kurtosis_roll_dumbbell)
training$kurtosis_picth_dumbbell  <- as.numeric(training$kurtosis_picth_dumbbell )
training$kurtosis_yaw_dumbbell <- as.numeric(training$kurtosis_yaw_dumbbell)
training$skewness_roll_dumbbell  <- as.numeric(training$skewness_roll_dumbbell)
training$skewness_pitch_dumbbell  <- as.numeric(training$skewness_pitch_dumbbell)
training$skewness_yaw_dumbbell  <- as.numeric(training$skewness_yaw_dumbbell)
training$max_yaw_dumbbell   <- as.numeric(training$max_yaw_dumbbell )
training$min_yaw_dumbbell  <- as.numeric(training$min_yaw_dumbbell)
training$amplitude_yaw_dumbbell  <- as.numeric(training$amplitude_yaw_dumbbell)
training$kurtosis_roll_forearm   <- as.numeric(training$kurtosis_roll_forearm )
training$kurtosis_picth_forearm  <- as.numeric(training$kurtosis_picth_forearm)
training$kurtosis_yaw_forearm  <- as.numeric(training$kurtosis_yaw_forearm)
training$skewness_roll_forearm  <- as.numeric(training$skewness_roll_forearm)
training$skewness_pitch_forearm  <- as.numeric(training$skewness_pitch_forearm)
training$skewness_yaw_forearm  <- as.numeric(training$skewness_yaw_forearm)
training$skewness_roll_forearm  <- as.numeric(training$skewness_roll_forearm)
training$skewness_pitch_forearm  <- as.numeric(training$skewness_pitch_forearm)
training$skewness_yaw_forearm  <- as.numeric(training$skewness_yaw_forearm)
training$max_yaw_forearm   <- as.numeric(training$max_yaw_forearm) 
training$min_yaw_forearm  <- as.numeric(training$min_yaw_forearm)
training$amplitude_yaw_forearm  <- as.numeric(training$amplitude_yaw_forearm)




LogicNA<-apply(training, 2, is.na)
Nas <- apply(LogicNA,2,sum)
Nas[Nas > 0]
```
Now i see that there are a lot of variables which have a lot of missing values 19000 of 19622 is a 97% so practically this columns are empty so i have to remove them
for that i will create a for loop witch is going to tell the number of the column that has empty values

```{r,cache=TRUE}
idx <- c()
for(i in 1:length(Nas)){
   if(Nas[i] > 19000){ idx[i] <- i}
}
idx <- idx[complete.cases(idx)]
idx
```
now with this index i can remove it from the training and test sets

```{r}
training <- training[,-idx]
testing <- testing[,-idx]
```
now, with this data, i am going to remove the first 7 rows because are more info about the person than the exercise
```{r}
training <- training[,-c(1:7)]
testing <- testing[,-c(1:7)]
dim(training)
dim(testing)
```
now i would like to use 3 the methods that were explained in the course, and see what happen
so for that first i am going to look the correlation between the predictors and see if through Principal component analysis i can reduce the number of predictors

```{r}
M <-  abs(cor(training[,-53]))
diag(M) <- 0
which(M > 0.80, arr.ind = T)
```
now we can see that there are a lot of realtionships betwwen some varibles so, for that whan i train a model i will put the preprocces with pca.
so now i would like to begin with the lm method or glm method, but how our outcome is a factor of a lot of levels doesn't work, so i will try with the next methods given by the course begiing with the tree

```{r,cache=TRUE}
set.seed(33)
library(rpart)
inTrain <- createDataPartition(y=training$classe,p=0.7,list=FALSE)
train <- training[inTrain,]
test<- training[-inTrain,]

model1 <- rpart(classe ~.,data=train)
predict1 <- predict(model1,test,type="class")
confusionMatrix(predict1,test$classe)
library(rattle)
fancyRpartPlot(model1)
```
now We can see that the accuracy rate of the model1 is good: 0.7341 and therefore the out-of-sample-error is about 0.2659 
now let's try with the random Forest

Note: the preProces of pca works better for regresion models, i tried pca with the tree and the accuracy was too low, without pca the accuracy works better, also with caret package the model was too slow and the accuracy too low

```{r,cache=TRUE}
set.seed(33)
library(randomForest)
model2 <- randomForest(x=train[,-53],y=train[,53])
predict2 <- predict(model2,test)
confusionMatrix(predict2,test$classe)
```
now We can see that the accuracy rate of the model 2 is really high: 0.9946 and therefore the out-of-sample-error is about 0.0054
now let's try with the naive Bayes method that assumes independence

```{r,cache=TRUE}
library(e1071)
set.seed(33)
model3 <- naiveBayes(train$classe ~ ., data=train)
predict3 <- predict(model3,test,type = "class")
confusionMatrix(predict3, test$classe)
```
now We can see that the accuracy rate of the model3 is medium: 0.5572 and therefore the out-of-sample-error is about 0.4428, so that means that the predictors have are not independent between them 

now i would like to prove with the gbm and ida method, but the caret package is taking too long so i wont use more models. so i am gonna to select the randomForest model to response the quizz because has the high accuracy 

so for that i am going to use the cross validation in that model
```{r,cache=TRUE}
library(caret)
library(randomForest)
numFolds <- trainControl(method = "cv", number = 10)
model2 <- randomForest(x=train[,-53],y=train[,53],trControl=numFolds)
predict2 <- predict(model2,test)
confusionMatrix(predict2,test$classe)
lastQuizz<- predict(model2, testing)
lastQuizz

```


